{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "lvu9i-NYbOmR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9873f359-06c9-44c1-9964-b73ee17331b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "lvu9i-NYbOmR"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "9a957e06-a7c1-420f-a588-2dfa79c979d0"
      },
      "outputs": [],
      "source": [
        "## IMPORTS\n",
        "import torch\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch import nn, Tensor,multinomial\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import TransformerEncoder, TransformerEncoderLayer, Transformer\n",
        "from torch.utils.data import dataset, DataLoader\n",
        "from torch.autograd import Variable\n",
        "from torchtext.vocab import Vectors, vocab,build_vocab_from_iterator,vocab\n",
        "\n",
        "import math\n",
        "import re\n",
        "import pickle\n",
        "from collections import defaultdict, OrderedDict, Counter\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from numpy.random import uniform\n",
        "import os"
      ],
      "id": "9a957e06-a7c1-420f-a588-2dfa79c979d0"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install transformers evaluate peft python-dotenv huggingface_hub wandb\n"
      ],
      "metadata": {
        "id": "Sa2gpi--q9JS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d05297d-dd20-4d61-9911-f38a51017852"
      },
      "id": "Sa2gpi--q9JS",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m78.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.4/81.4 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m87.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m91.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m474.6/474.6 kB\u001b[0m \u001b[31m48.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.5/212.5 kB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m219.1/219.1 kB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 kB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m206.5/206.5 kB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m74.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "zaV1SMr9LG18"
      },
      "outputs": [],
      "source": [
        "## CONSTANTS\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "lang2code = {\n",
        "    \"Gitksan\" : \"git\", \n",
        "    \"Arapaho\" : \"arp\", \n",
        "    \"Lezgi\" : \"lez\", \n",
        "    \"Nyangbo\" : \"nyb\", \n",
        "    \"Tsez\" : \"ddo\",\n",
        "    \"Uspanteko\" : \"usp\",  \n",
        "    \"Natugu\" : \"ntu\"\n",
        "    }\n",
        "\n",
        "code2lang = {v:k for k,v in lang2code.items()}\n",
        "\n",
        "START = \"<start>\"\n",
        "END = \"<end>\"\n",
        "UNK = \"<unk>\"\n",
        "PAD = \"<pad>\"\n",
        "SP = \"#\"\n",
        "SPECIALS = [PAD, START, END,UNK,SP, \"?\",\"!\", \"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"0\"]\n",
        "\n",
        "PATH = \"/content/drive/MyDrive/Colab_Notebooks/2023glossingST/\" \n",
        "\n",
        "LOAD_FROM_CKPT = False\n"
      ],
      "id": "zaV1SMr9LG18"
    },
    {
      "cell_type": "code",
      "source": [
        "# from dotenv import load_dotenv\n",
        "# load_dotenv(\"/content/drive/MyDrive/Colab_Notebooks/.env\")\n",
        "# HF = os.getenv(\"HF_TOKEN\")\n",
        "# WANDB = os.getenv(\"WANDB_TOKEN\")\n",
        "\n",
        "# !huggingface-cli login --token $HF\n",
        "# !wandb login $WANDB"
      ],
      "metadata": {
        "id": "kvduBlVNq5U-"
      },
      "id": "kvduBlVNq5U-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing"
      ],
      "metadata": {
        "id": "_1BK9tWXRV-_"
      },
      "id": "_1BK9tWXRV-_"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Helpers"
      ],
      "metadata": {
        "id": "YSIg0JyTJeAu"
      },
      "id": "YSIg0JyTJeAu"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def tag_bd(seq:list):\n",
        "  '''\n",
        "  Attaches boundary tokens to previous and following characters in tokenized gloss w/ character-level tokenization\n",
        "  to preserve delimiters.\n",
        "\n",
        "  E.g. [\"I\", \"N\" - \"s\"] -> [\"I\", \"N-\", \"-s\"]\n",
        "  '''\n",
        "  BDS = {\"-\", \".\"}\n",
        "  for i, c in enumerate(seq):\n",
        "    if i < len(seq)-2:\n",
        "      if seq[i+1] in BDS:\n",
        "          seq[i] += seq[i+1]\n",
        "          seq[i+2] = seq[i+1] + seq[i+2]\n",
        "  return [i for i in seq if not i in BDS]\n",
        "\n",
        "# Preprocessing pipelines \n",
        "orth_process = lambda x: [START] + list(re.sub(\" \", SP, x)) + [END]              # source (transcription) line\n",
        "gloss_process = lambda x: [START] + tag_bd(list(re.sub(\" \", SP, x))) + [END]     # target (gloss) line\n",
        "\n",
        "\n",
        "def get_linesplits(orths:list, \n",
        "                   glosses:list):\n",
        "  ex = [(orth_process(o), gloss_process(g)) for o, g in zip(orths, glosses)]\n",
        "  return ex"
      ],
      "metadata": {
        "id": "TOuZYZOZRCSp"
      },
      "id": "TOuZYZOZRCSp",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simple segmentation-based augmentation"
      ],
      "metadata": {
        "id": "J4cqrPzRRFlz"
      },
      "id": "J4cqrPzRRFlz"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "0d1f0d13-c1d2-43c8-8024-9b844a87ce4e"
      },
      "outputs": [],
      "source": [
        "def get_windows(glosses:list,prop=0.7):\n",
        "    '''\n",
        "    Randomly samples window sizes from uniform distribution \n",
        "    upper length thresholds based on training gloss lengths, modulated by parameter `prop`\n",
        "    lower length threshold = 1, to ensure word-level representation \n",
        "    '''\n",
        "    counts=defaultdict(int) # Count frequency of gloss lengths \n",
        "\n",
        "    for gloss in glosses:\n",
        "        counts[len(gloss.split())] += 1\n",
        "    \n",
        "    sorted_counts = sorted(counts.items(),key=lambda x:x[1],reverse=True)\n",
        "\n",
        "    lcounts = [l for l,c in sorted_counts]\n",
        "    avg = round(sum(lcounts)/len(lcounts)) * prop\n",
        "    \n",
        "    while True:\n",
        "        windows = np.random.randint(1,avg,int(avg))\n",
        "        if len(set(windows)) == int(avg)-1:\n",
        "            break\n",
        "    return set(windows)\n",
        "\n",
        "def get_window_segs(transcriptions:list, glosses:list, windows:list):\n",
        "    '''\n",
        "    Generate synthetic training data using sliding-window segmentation on pre-existing set.\n",
        "    '''\n",
        "    window_segs = []\n",
        "    for trans, gls in zip(transcriptions,glosses):\n",
        "        transsplit = trans.split()\n",
        "        glssplit = gls.split()\n",
        "        for window in windows:\n",
        "            if len(glssplit) < window:\n",
        "                continue\n",
        "            sid = 0\n",
        "            for i in range(len(glssplit)-1):\n",
        "                transwind = transsplit[sid:sid+window]\n",
        "                glsswind = glssplit[sid:sid+window]\n",
        "                if len(transwind) < window:\n",
        "                    continue\n",
        "                window_segs.append((orth_process(\" \".join(transwind)),gloss_process(\" \".join(glsswind))))\n",
        "\n",
        "                sid+=1\n",
        "    return window_segs\n",
        "        "
      ],
      "id": "0d1f0d13-c1d2-43c8-8024-9b844a87ce4e"
    },
    {
      "cell_type": "code",
      "source": [
        "class LangIGT:\n",
        "    '''\n",
        "    Data from a single language, processed for sequence-to-sequence transduction.\n",
        "    '''\n",
        "    def __init__(self, lang):\n",
        "        self.lang = lang\n",
        "        self.train_ld, self.dev_ld, self.test_ld = self.get_langdict()\n",
        "        self.windows = get_windows(self.train_ld[\"glosses\"])\n",
        "        self.train_splits = get_linesplits(self.train_ld[\"transcriptions\"],self.train_ld[\"glosses\"])\n",
        "        self.dev_splits = get_linesplits(self.dev_ld[\"transcriptions\"],self.dev_ld[\"glosses\"])\n",
        "        self.test_splits = get_linesplits(self.test_ld[\"transcriptions\"],self.test_ld[\"glosses\"])\n",
        "        self.train_window_segs = get_window_segs(self.train_ld[\"transcriptions\"],self.train_ld[\"glosses\"], self.windows)\n",
        "        self.gls_alphabet, self.orth_alphabet = self.get_alphabet()\n",
        "        \n",
        "    \n",
        "    def get_langdict(self):\n",
        "        ld = {\n",
        "            \"train\": defaultdict(list),\n",
        "            \"dev\": defaultdict(list),\n",
        "            \"test\" : defaultdict(list)\n",
        "        }\n",
        "        path = PATH + \"data/\" + self.lang +\"/\"\n",
        "        #path = f\"data/{self.lang}/\"\n",
        "\n",
        "        for split in [\"train\", \"dev\", \"test\"]:\n",
        "            fp = f\"{lang2code[self.lang]}-{split}-track1-uncovered\"\n",
        "            with open(path + fp, \"r\", encoding=\"utf-8\") as f:\n",
        "                for line in f.readlines():\n",
        "                    if line.startswith(\"\\\\t\"):\n",
        "                        ld[split][\"transcriptions\"].append(line.lstrip(\"\\\\t \").rstrip(\"\\n\"))\n",
        "                    if line.startswith(\"\\g\"):\n",
        "                        ld[split][\"glosses\"].append(line.lstrip(\"\\\\g \").rstrip(\"\\n\"))\n",
        "                    if line.startswith(\"\\l\"):\n",
        "                        ld[split][\"translation\"].append(line.lstrip(\"\\\\l \").rstrip(\"\\n\"))\n",
        "        \n",
        "        return ld[\"train\"], ld[\"dev\"], ld[\"test\"]\n",
        "\n",
        "    def get_alphabet(self):\n",
        "      gloss_counter, orth_counter = Counter(), Counter()\n",
        "\n",
        "      for gloss,trans in zip(self.train_ld[\"glosses\"],self.train_ld[\"transcriptions\"]):\n",
        "        for g,t in zip(tag_bd(list(re.sub(\" \", \"#\", gloss))), list(re.sub(\" \", \"#\", trans))):\n",
        "          gloss_counter.update([g])\n",
        "          orth_counter.update([t])\n",
        "\n",
        "\n",
        "      gloss_voc = vocab(OrderedDict(sorted(gloss_counter.items(), key=lambda x: x[1], reverse=True)), specials=SPECIALS)\n",
        "      gloss_voc.set_default_index(gloss_voc[UNK])\n",
        "      orth_voc = vocab(OrderedDict(sorted(orth_counter.items(), key=lambda x: x[1], reverse=True)), specials=SPECIALS)\n",
        "      orth_voc.set_default_index(orth_voc[UNK])\n",
        "\n",
        "      return gloss_voc, orth_voc\n",
        "\n",
        "    def data(self,split):\n",
        "\n",
        "      train = [i for i in self.train_splits] + [i for i in self.train_window_segs] # Augment just the training set\n",
        "      data = {\n",
        "            \"train\" : train,\n",
        "            \"dev\" : self.dev_splits,\n",
        "            \"test\" : self.test_splits\n",
        "        }\n",
        "      return data[split]\n",
        "\n",
        "    def alphabet(self,split):\n",
        "      alphabet = {\n",
        "            \"gloss\" : self.gls_alphabet,\n",
        "            \"orth\" : self.orth_alphabet\n",
        "        }\n",
        "      return alphabet[split]"
      ],
      "metadata": {
        "id": "2KHxYtm2Ruwz"
      },
      "id": "2KHxYtm2Ruwz",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "p5s_A0L2kXWT"
      },
      "outputs": [],
      "source": [
        "# Initialize data for given language\n",
        "CODE = \"git\" \n",
        "lang = LangIGT(code2lang[CODE]) \n",
        "MAXLEN = max([len(i) for _,i in lang.train_splits])"
      ],
      "id": "p5s_A0L2kXWT"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QtpSr8eOnulQ",
        "outputId": "38b277a3-f500-47f0-d32f-b1227d43d3f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "window sizes: {1, 2, 3, 4}\n",
            "size of training set: 858\n",
            "size of dev set: 42\n",
            "size of test set: 37\n",
            "longest gloss length in train set: 120\n"
          ]
        }
      ],
      "source": [
        "print(f\"window sizes: {lang.windows}\")\n",
        "print(f\"size of training set: {len(lang.data('train'))}\")\n",
        "print(f\"size of dev set: {len(lang.data('dev'))}\")\n",
        "print(f\"size of test set: {len(lang.data('test'))}\")\n",
        "\n",
        "print(f\"longest gloss length in train set: {MAXLEN}\")\n"
      ],
      "id": "QtpSr8eOnulQ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tensor transformations for batched training\n",
        "\n"
      ],
      "metadata": {
        "id": "G6g-pFSPTENv"
      },
      "id": "G6g-pFSPTENv"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "a44851c7-8373-449f-a2e8-7ee6ba05da27"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 32\n",
        "\n",
        "def collate_batch(batch):\n",
        "    input, output=[], []\n",
        "    for morphemes, glosses in batch:\n",
        "        input_tensor = torch.tensor([orth_alpha[i] for i in morphemes], dtype=torch.long)\n",
        "        output_tensor = torch.tensor([gloss_alpha[i] for i in glosses], dtype=torch.long)\n",
        "        input.append(input_tensor)\n",
        "        output.append(output_tensor)\n",
        "\n",
        "    return pad_sequence(input, \n",
        "                        batch_first=False, \n",
        "                        padding_value=orth_alpha[PAD]), pad_sequence(output, \n",
        "                        batch_first=False, \n",
        "                        padding_value=gloss_alpha[PAD])\n",
        "\n",
        "\n",
        "def torchify(train,dev,test):    \n",
        "    train_dataloader = DataLoader(\n",
        "        train,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        collate_fn=collate_batch,\n",
        "        shuffle=True\n",
        "    )\n",
        "    dev_dataloader = DataLoader(\n",
        "        dev,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        collate_fn=collate_batch,\n",
        "        shuffle=True\n",
        "    )\n",
        "    test_dataloader = DataLoader(\n",
        "        test,\n",
        "        batch_size=1,\n",
        "        collate_fn=collate_batch,\n",
        "        shuffle=False\n",
        "    )\n",
        "    return train_dataloader, dev_dataloader, test_dataloader\n",
        "\n",
        "\n",
        "if LOAD_FROM_CKPT:\n",
        "\n",
        "  train_dataloader = pickle.load(open(PATH + f\"dataloaders/{CODE}_train_dataloader\", \"rb\"))\n",
        "  dev_dataloader = pickle.load(open(PATH + f\"dataloaders/{CODE}_dev_dataloader\", \"rb\"))\n",
        "  test_dataloader = pickle.load(open(PATH + f\"dataloaders/{CODE}_test_dataloader\", \"rb\"))\n",
        "  gloss_alpha = pickle.load(open(PATH + f\"dataloaders/{CODE}_trg_alpha\", \"rb\"))\n",
        "  orth_alpha = pickle.load(open(PATH + f\"dataloaders/{CODE}_src_alpha\", \"rb\"))  \n",
        "\n",
        "else:\n",
        "  train, dev, test = lang.data(\"train\"), lang.data(\"dev\"), lang.data(\"test\")\n",
        "  gloss_alpha, orth_alpha = lang.alphabet(\"gloss\"), lang.alphabet(\"orth\")\n",
        "  loader_path = PATH + \"dataloaders/\"\n",
        "\n",
        "  train_dataloader, dev_dataloader, test_dataloader = torchify(train, dev, test)\n",
        "\n",
        "  ## Save for reproducibility\n",
        "\n",
        "  with open(PATH+f\"dataloaders/{CODE}_train_dataloader\", \"wb\") as f:\n",
        "      pickle.dump(train_dataloader, f)\n",
        "\n",
        "  with open(PATH+f\"dataloaders/{CODE}_dev_dataloader\", \"wb\") as f:\n",
        "      pickle.dump(dev_dataloader, f)\n",
        "\n",
        "  with open(PATH+f\"dataloaders/{CODE}_test_dataloader\", \"wb\") as f:\n",
        "      pickle.dump(test_dataloader, f)\n",
        "\n",
        "  with open(PATH+f\"dataloaders/{CODE}_src_alpha\", \"wb\") as f:\n",
        "      pickle.dump(orth_alpha, f)\n",
        "\n",
        "  with open(PATH+f\"dataloaders/{CODE}_trg_alpha\", \"wb\") as f:\n",
        "      pickle.dump(gloss_alpha, f)"
      ],
      "id": "a44851c7-8373-449f-a2e8-7ee6ba05da27"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4hhcxQda5wL",
        "outputId": "9daf0728-9989-4091-f844-4ffbb02a71f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "source tensor size: torch.Size([40, 32])\n",
            "target tensor size: torch.Size([42, 32])\n",
            "the tensor of first example in target: tensor([ 1, 18, 18, 17, 26,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0])\n",
            "the tensor of first example in src: tensor([ 1, 37, 17,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0])\n",
            "['<start>', 'I', 'i', '<end>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
            "['<start>', 'C', 'C', 'N', 'J', '<end>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n"
          ]
        }
      ],
      "source": [
        "## Make sure data looks ok\n",
        "for batch in train_dataloader:\n",
        "    src, trg = batch\n",
        "    print('source tensor size:', src.shape)\n",
        "    print('target tensor size:', trg.shape)\n",
        "    print('the tensor of first example in target:', trg[:, 0])\n",
        "    print('the tensor of first example in src:', src[:, 0])\n",
        "    print([orth_alpha.get_itos()[i] for i in src[:, 0]])\n",
        "    print([gloss_alpha.get_itos()[i] for i in trg[:, 0]])\n",
        "    break"
      ],
      "id": "b4hhcxQda5wL"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model building \n",
        "## Architecture: PyTorch TransformerModel"
      ],
      "metadata": {
        "id": "x3RVnVMkTo0W"
      },
      "id": "x3RVnVMkTo0W"
    },
    {
      "cell_type": "code",
      "source": [
        "## HYPERPARAMETERS\n",
        "SRC_VOCAB_SIZE = len(orth_alpha)\n",
        "TGT_VOCAB_SIZE = len(gloss_alpha)\n",
        "EMB_SIZE = 300  \n",
        "NHEAD = 6\n",
        "FFN_HID_DIM = 512\n",
        "NUM_ENCODER_LAYERS = 3\n",
        "NUM_DECODER_LAYERS = 3\n",
        "NUM_EPOCHS = 50\n",
        "DROPOUT = 0.25"
      ],
      "metadata": {
        "id": "zqR8pxdFINf-"
      },
      "id": "zqR8pxdFINf-",
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SRC_VOCAB_SIZE"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F9AYW8ZFzq3V",
        "outputId": "ad3e617a-2129-42e4-df3e-3e04f41e9d4b"
      },
      "id": "F9AYW8ZFzq3V",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "55"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "458e3721-f928-4816-93ba-c12cefef4c72"
      },
      "outputs": [],
      "source": [
        "\n",
        "# helper Module that adds positional encoding to the token embedding to introduce a notion of word order.\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self,\n",
        "                 emb_size: int,\n",
        "                 dropout: float,\n",
        "                 maxlen: int = MAXLEN + 300):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n",
        "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
        "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
        "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
        "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
        "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer('pos_embedding', pos_embedding)\n",
        "\n",
        "    def forward(self, token_embedding: Tensor):\n",
        "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])\n",
        "\n",
        "# helper Module to convert tensor of input indices into corresponding tensor of token embeddings\n",
        "class TokenEmbedding(nn.Module):\n",
        "    def __init__(self, emb_size, embedding: Vectors):\n",
        "        super(TokenEmbedding, self).__init__()\n",
        "        #self.embedding = nn.Embedding(vocab_size, emb_size)\n",
        "        self.embed = embedding\n",
        "        #self.emb_size = emb_size\n",
        "        self.emb_size = emb_size\n",
        "\n",
        "    def forward(self, tokens: Tensor):\n",
        "        return self.embed(tokens.long()) * math.sqrt(self.emb_size)\n",
        "\n",
        "# Seq2Seq Network\n",
        "class Seq2SeqTransformer(nn.Module):\n",
        "    def __init__(self,\n",
        "                 num_encoder_layers: int,\n",
        "                 num_decoder_layers: int,\n",
        "                 emb_size: int,\n",
        "                 nhead: int,\n",
        "                 src_vocab_size: int,\n",
        "                 tgt_vocab_size: int,\n",
        "                 src_embeddings: nn.Embedding,\n",
        "                 tgt_embeddings: Vectors,\n",
        "                 dim_feedforward: int = 512,\n",
        "                 dropout: float = DROPOUT):\n",
        "        super(Seq2SeqTransformer, self).__init__()\n",
        "        self.transformer = Transformer(d_model=emb_size,\n",
        "                                       nhead=nhead,\n",
        "                                       num_encoder_layers=num_encoder_layers,\n",
        "                                       num_decoder_layers=num_decoder_layers,\n",
        "                                       dim_feedforward=dim_feedforward,\n",
        "                                       dropout=dropout)\n",
        "        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n",
        "        self.src_tok_emb = TokenEmbedding(emb_size, src_embeddings)\n",
        "        self.tgt_tok_emb = TokenEmbedding(emb_size, tgt_embeddings)\n",
        "        self.positional_encoding = PositionalEncoding(\n",
        "            emb_size, dropout=dropout)\n",
        "\n",
        "    def forward(self,\n",
        "                src: Tensor,\n",
        "                trg: Tensor,\n",
        "                src_mask: Tensor,\n",
        "                tgt_mask: Tensor,\n",
        "                src_padding_mask: Tensor,\n",
        "                tgt_padding_mask: Tensor,\n",
        "                memory_key_padding_mask: Tensor):\n",
        "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
        "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n",
        "        outs = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask, None,\n",
        "                                src_padding_mask, tgt_padding_mask, memory_key_padding_mask)\n",
        "        return self.generator(outs)\n",
        "\n",
        "    def encode(self, src: Tensor, src_mask: Tensor):\n",
        "        return self.transformer.encoder(self.positional_encoding(\n",
        "                            self.src_tok_emb(src)), src_mask)\n",
        "\n",
        "    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):\n",
        "        return self.transformer.decoder(self.positional_encoding(\n",
        "                          self.tgt_tok_emb(tgt)), memory,\n",
        "                          tgt_mask)"
      ],
      "id": "458e3721-f928-4816-93ba-c12cefef4c72"
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "mT7MGqLIrN73"
      },
      "outputs": [],
      "source": [
        "def generate_square_subsequent_mask(sz):\n",
        "    mask = (torch.triu(torch.ones((sz, sz), device=DEVICE)) == 1).transpose(0, 1)\n",
        "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "    return mask\n",
        "\n",
        "\n",
        "def create_mask(src, tgt):\n",
        "    src_seq_len = src.shape[0]\n",
        "    tgt_seq_len = tgt.shape[0]\n",
        "\n",
        "    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n",
        "    src_mask = torch.zeros((src_seq_len, src_seq_len),device=DEVICE).type(torch.bool)\n",
        "\n",
        "    src_padding_mask = (src == orth_alpha.get_stoi()[PAD]).transpose(0, 1)\n",
        "    tgt_padding_mask = (tgt == gloss_alpha.get_stoi()[PAD]).transpose(0, 1)\n",
        "    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask"
      ],
      "id": "mT7MGqLIrN73"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training and inference"
      ],
      "metadata": {
        "id": "UhQt7hDFT-z_"
      },
      "id": "UhQt7hDFT-z_"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialize parameters"
      ],
      "metadata": {
        "id": "Lb4UK-O3UNwr"
      },
      "id": "Lb4UK-O3UNwr"
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "wu1XtSqprWSf"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(531)\n",
        "\n",
        "src_embeddings = nn.Embedding(SRC_VOCAB_SIZE, EMB_SIZE)\n",
        "tgt_embeddings = nn.Embedding(TGT_VOCAB_SIZE, EMB_SIZE)\n",
        "\n",
        "\n",
        "transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE,\n",
        "                                 NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, src_embeddings,\n",
        "                                 tgt_embeddings, FFN_HID_DIM)\n",
        "\n",
        "for p in transformer.parameters():\n",
        "    if p.dim() > 1:\n",
        "        nn.init.xavier_uniform_(p)\n",
        "\n",
        "transformer = transformer.to(DEVICE)\n",
        "\n",
        "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=0)\n",
        "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)"
      ],
      "id": "wu1XtSqprWSf"
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "DBMe5q6Hrsdn"
      },
      "outputs": [],
      "source": [
        "def train_epoch(model, iterator, optimizer):\n",
        "    model.train()\n",
        "    losses = 0\n",
        "\n",
        "    for src, tgt in tqdm(iterator):\n",
        "        src = src.to(DEVICE)\n",
        "        tgt = tgt.to(DEVICE)\n",
        "\n",
        "        tgt_input = tgt[:-1, :]\n",
        "\n",
        "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
        "\n",
        "        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        tgt_out = tgt[1:, :]\n",
        "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        losses += loss.item()\n",
        "\n",
        "    return losses / len(iterator)\n",
        "\n",
        "\n",
        "def evaluate(model, iterator):\n",
        "    model.eval()\n",
        "    losses = 0\n",
        "\n",
        "    for src, tgt in iterator:\n",
        "        src = src.to(DEVICE)\n",
        "        tgt = tgt.to(DEVICE)\n",
        "\n",
        "        tgt_input = tgt[:-1, :]\n",
        "\n",
        "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
        "\n",
        "        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
        "\n",
        "        tgt_out = tgt[1:, :]\n",
        "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
        "        losses += loss.item()\n",
        "\n",
        "    return losses / len(iterator)"
      ],
      "id": "DBMe5q6Hrsdn"
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "4lT_OG7arysQ"
      },
      "outputs": [],
      "source": [
        "from timeit import default_timer as timer\n",
        "import pandas as pd\n",
        "\n",
        "def train_eval(NUM_EPOCHS, transformer, train_dataloader, optimizer, dev_dataloader, lang_code):\n",
        "    losses = defaultdict(list)\n",
        "    for epoch in range(1, NUM_EPOCHS + 1):\n",
        "        prev_loss = 0\n",
        "        start_time = timer()\n",
        "        train_loss = train_epoch(transformer, train_dataloader, optimizer)\n",
        "        end_time = timer()\n",
        "        val_loss = evaluate(transformer, dev_dataloader)\n",
        "\n",
        "        if epoch % 2 == 0:\n",
        "\n",
        "          torch.save(transformer.state_dict(),\n",
        "                     PATH + \"models/\" + lang_code + str(epoch))\n",
        "          print(f\"Checkpoint saved at epoch={epoch}\")\n",
        "\n",
        "        print((f\"Epoch: {epoch},  \"f\"Epoch time = {(end_time - start_time):.3f}s\"))\n",
        "        print(f'\\t Train Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "        print(f'\\t Val. Loss: {val_loss:.3f} |  Val. PPL: {math.exp(val_loss):7.3f}')\n",
        "\n",
        "        losses[\"train_loss\"].append(train_loss)\n",
        "        losses[\"val_loss\"].append(val_loss)\n",
        "        losses_df = pd.DataFrame.from_dict(losses)\n",
        "        losses_df.to_csv(PATH + \"losses/\" + lang_code +\".csv\")\n",
        "    return losses\n"
      ],
      "id": "4lT_OG7arysQ"
    },
    {
      "cell_type": "code",
      "source": [
        "losses = train_eval(30, transformer, train_dataloader, optimizer, dev_dataloader, CODE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z31_a5LVZvfL",
        "outputId": "0aad8206-6158-4c76-b59c-2c7e3e2b54a6"
      },
      "id": "z31_a5LVZvfL",
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:01<00:00, 25.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1,  Epoch time = 1.057s\n",
            "\t Train Loss: 4.401 | Train PPL:  81.506\n",
            "\t Val. Loss: 3.981 |  Val. PPL:  53.594\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/27 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n",
            "100%|██████████| 27/27 [00:00<00:00, 28.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint saved at epoch=2\n",
            "Epoch: 2,  Epoch time = 0.950s\n",
            "\t Train Loss: 3.933 | Train PPL:  51.034\n",
            "\t Val. Loss: 3.539 |  Val. PPL:  34.423\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/27 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n",
            "100%|██████████| 27/27 [00:00<00:00, 29.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 3,  Epoch time = 0.926s\n",
            "\t Train Loss: 3.462 | Train PPL:  31.870\n",
            "\t Val. Loss: 3.222 |  Val. PPL:  25.084\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/27 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n",
            "100%|██████████| 27/27 [00:00<00:00, 28.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint saved at epoch=4\n",
            "Epoch: 4,  Epoch time = 0.950s\n",
            "\t Train Loss: 3.054 | Train PPL:  21.194\n",
            "\t Val. Loss: 3.107 |  Val. PPL:  22.361\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/27 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n",
            "100%|██████████| 27/27 [00:00<00:00, 28.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 5,  Epoch time = 0.955s\n",
            "\t Train Loss: 2.744 | Train PPL:  15.544\n",
            "\t Val. Loss: 3.068 |  Val. PPL:  21.502\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/27 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n",
            "100%|██████████| 27/27 [00:00<00:00, 27.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint saved at epoch=6\n",
            "Epoch: 6,  Epoch time = 0.975s\n",
            "\t Train Loss: 2.509 | Train PPL:  12.290\n",
            "\t Val. Loss: 2.967 |  Val. PPL:  19.433\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/27 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n",
            "100%|██████████| 27/27 [00:00<00:00, 27.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 7,  Epoch time = 0.980s\n",
            "\t Train Loss: 2.311 | Train PPL:  10.084\n",
            "\t Val. Loss: 2.846 |  Val. PPL:  17.213\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/27 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n",
            "100%|██████████| 27/27 [00:01<00:00, 26.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint saved at epoch=8\n",
            "Epoch: 8,  Epoch time = 1.042s\n",
            "\t Train Loss: 2.160 | Train PPL:   8.673\n",
            "\t Val. Loss: 2.823 |  Val. PPL:  16.819\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/27 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n",
            "100%|██████████| 27/27 [00:00<00:00, 28.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 9,  Epoch time = 0.946s\n",
            "\t Train Loss: 2.026 | Train PPL:   7.583\n",
            "\t Val. Loss: 2.999 |  Val. PPL:  20.061\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/27 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n",
            "100%|██████████| 27/27 [00:00<00:00, 28.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint saved at epoch=10\n",
            "Epoch: 10,  Epoch time = 0.949s\n",
            "\t Train Loss: 1.915 | Train PPL:   6.789\n",
            "\t Val. Loss: 2.869 |  Val. PPL:  17.615\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/27 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n",
            "100%|██████████| 27/27 [00:01<00:00, 26.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 11,  Epoch time = 1.015s\n",
            "\t Train Loss: 1.814 | Train PPL:   6.138\n",
            "\t Val. Loss: 2.901 |  Val. PPL:  18.191\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/27 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n",
            "100%|██████████| 27/27 [00:00<00:00, 27.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint saved at epoch=12\n",
            "Epoch: 12,  Epoch time = 0.977s\n",
            "\t Train Loss: 1.716 | Train PPL:   5.563\n",
            "\t Val. Loss: 2.811 |  Val. PPL:  16.634\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/27 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n",
            "100%|██████████| 27/27 [00:00<00:00, 27.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 13,  Epoch time = 1.010s\n",
            "\t Train Loss: 1.666 | Train PPL:   5.290\n",
            "\t Val. Loss: 2.950 |  Val. PPL:  19.114\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/27 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n",
            "100%|██████████| 27/27 [00:00<00:00, 27.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint saved at epoch=14\n",
            "Epoch: 14,  Epoch time = 0.979s\n",
            "\t Train Loss: 1.583 | Train PPL:   4.869\n",
            "\t Val. Loss: 2.827 |  Val. PPL:  16.900\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/27 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n",
            "100%|██████████| 27/27 [00:00<00:00, 28.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 15,  Epoch time = 0.959s\n",
            "\t Train Loss: 1.531 | Train PPL:   4.624\n",
            "\t Val. Loss: 2.870 |  Val. PPL:  17.633\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/27 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n",
            "100%|██████████| 27/27 [00:00<00:00, 28.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint saved at epoch=16\n",
            "Epoch: 16,  Epoch time = 0.946s\n",
            "\t Train Loss: 1.454 | Train PPL:   4.279\n",
            "\t Val. Loss: 2.841 |  Val. PPL:  17.137\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/27 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n",
            "100%|██████████| 27/27 [00:00<00:00, 28.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 17,  Epoch time = 0.965s\n",
            "\t Train Loss: 1.414 | Train PPL:   4.114\n",
            "\t Val. Loss: 2.867 |  Val. PPL:  17.579\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/27 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n",
            "100%|██████████| 27/27 [00:00<00:00, 27.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint saved at epoch=18\n",
            "Epoch: 18,  Epoch time = 0.984s\n",
            "\t Train Loss: 1.375 | Train PPL:   3.955\n",
            "\t Val. Loss: 2.793 |  Val. PPL:  16.335\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/27 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n",
            "100%|██████████| 27/27 [00:00<00:00, 28.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 19,  Epoch time = 0.951s\n",
            "\t Train Loss: 1.312 | Train PPL:   3.712\n",
            "\t Val. Loss: 2.776 |  Val. PPL:  16.051\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/27 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n",
            "100%|██████████| 27/27 [00:00<00:00, 28.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint saved at epoch=20\n",
            "Epoch: 20,  Epoch time = 0.961s\n",
            "\t Train Loss: 1.269 | Train PPL:   3.556\n",
            "\t Val. Loss: 2.838 |  Val. PPL:  17.074\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/27 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n",
            "100%|██████████| 27/27 [00:00<00:00, 27.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 21,  Epoch time = 0.975s\n",
            "\t Train Loss: 1.222 | Train PPL:   3.395\n",
            "\t Val. Loss: 2.806 |  Val. PPL:  16.548\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/27 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n",
            "100%|██████████| 27/27 [00:00<00:00, 27.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint saved at epoch=22\n",
            "Epoch: 22,  Epoch time = 0.995s\n",
            "\t Train Loss: 1.180 | Train PPL:   3.254\n",
            "\t Val. Loss: 2.893 |  Val. PPL:  18.039\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/27 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n",
            "100%|██████████| 27/27 [00:00<00:00, 27.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 23,  Epoch time = 0.994s\n",
            "\t Train Loss: 1.144 | Train PPL:   3.139\n",
            "\t Val. Loss: 2.877 |  Val. PPL:  17.760\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/27 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n",
            "100%|██████████| 27/27 [00:00<00:00, 27.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint saved at epoch=24\n",
            "Epoch: 24,  Epoch time = 0.988s\n",
            "\t Train Loss: 1.112 | Train PPL:   3.041\n",
            "\t Val. Loss: 2.992 |  Val. PPL:  19.934\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/27 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n",
            "100%|██████████| 27/27 [00:00<00:00, 27.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 25,  Epoch time = 0.976s\n",
            "\t Train Loss: 1.079 | Train PPL:   2.941\n",
            "\t Val. Loss: 2.887 |  Val. PPL:  17.945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/27 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n",
            "100%|██████████| 27/27 [00:00<00:00, 27.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint saved at epoch=26\n",
            "Epoch: 26,  Epoch time = 0.991s\n",
            "\t Train Loss: 1.046 | Train PPL:   2.846\n",
            "\t Val. Loss: 3.021 |  Val. PPL:  20.515\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/27 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n",
            "100%|██████████| 27/27 [00:00<00:00, 27.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 27,  Epoch time = 0.974s\n",
            "\t Train Loss: 1.007 | Train PPL:   2.736\n",
            "\t Val. Loss: 2.944 |  Val. PPL:  18.996\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/27 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n",
            "100%|██████████| 27/27 [00:00<00:00, 27.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint saved at epoch=28\n",
            "Epoch: 28,  Epoch time = 0.984s\n",
            "\t Train Loss: 0.984 | Train PPL:   2.674\n",
            "\t Val. Loss: 3.002 |  Val. PPL:  20.126\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/27 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n",
            "100%|██████████| 27/27 [00:00<00:00, 28.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 29,  Epoch time = 0.954s\n",
            "\t Train Loss: 0.958 | Train PPL:   2.607\n",
            "\t Val. Loss: 3.048 |  Val. PPL:  21.077\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/27 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n",
            "100%|██████████| 27/27 [00:00<00:00, 27.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint saved at epoch=30\n",
            "Epoch: 30,  Epoch time = 0.992s\n",
            "\t Train Loss: 0.921 | Train PPL:   2.512\n",
            "\t Val. Loss: 3.094 |  Val. PPL:  22.068\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_tensors = [src for src, _ in test_dataloader]\n",
        "transformer_best = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE,\n",
        "                                NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, src_embeddings,\n",
        "                                tgt_embeddings, FFN_HID_DIM)\n",
        "\n",
        "ckpt = torch.load(PATH + f'models/{CODE}{22}',map_location=DEVICE)\n",
        "transformer_best.load_state_dict(ckpt,strict=False)\n",
        "transformer_best = transformer_best.to(DEVICE)"
      ],
      "metadata": {
        "id": "DJ7wc-ABpqOH"
      },
      "id": "DJ7wc-ABpqOH",
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "30-ZSS8dr8XQ"
      },
      "outputs": [],
      "source": [
        "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
        "    src = src.to(DEVICE)\n",
        "    src_mask = src_mask.to(DEVICE)\n",
        "\n",
        "    memory = model.encode(src, src_mask)\n",
        "    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n",
        "    for i in range(max_len+10):\n",
        "        memory = memory.to(DEVICE)\n",
        "        tgt_mask = (generate_square_subsequent_mask(ys.size(0))\n",
        "                    .type(torch.bool)).to(DEVICE)\n",
        "        out = model.decode(ys, memory, tgt_mask)\n",
        "        out = out.transpose(0, 1)\n",
        "        prob = model.generator(out[:, -1])\n",
        "        _, next_word = torch.max(prob, dim=1)\n",
        "        next_word = next_word.item()\n",
        "\n",
        "        ys = torch.cat([ys,\n",
        "                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)\n",
        "        if next_word == gloss_alpha.get_stoi()[END]:\n",
        "            break\n",
        "    return ys\n",
        "\n",
        "# actual function to translate input sentence into target language\n",
        "def translate(model: torch.nn.Module, src_tensor, extend_by=50):\n",
        "    model.eval()\n",
        "    src = src_tensor.view(-1, 1)\n",
        "    num_tokens = src.shape[0]\n",
        "    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n",
        "    tgt_tokens = greedy_decode(\n",
        "        model,  \n",
        "        src, \n",
        "        src_mask, \n",
        "        max_len=num_tokens + extend_by, \n",
        "        start_symbol=orth_alpha.get_stoi()[START]\n",
        "    ).flatten()\n",
        "\n",
        "    return re.sub(\"<start>|<end>\", \"\", ''.join([gloss_alpha.get_itos()[t] for t in tgt_tokens]))#\" \".join(vocab_transform[TGT_LANGUAGE].lookup_tokens(list(tgt_tokens.cpu().numpy()))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\")"
      ],
      "id": "30-ZSS8dr8XQ"
    },
    {
      "cell_type": "code",
      "source": [
        "PRED_PATH = \"/content/drive/MyDrive/Colab_Notebooks/2023glossingST/final_preds/\"\n",
        "def get_predictions(transformer_best, test_tensors):\n",
        "  test_tensors = [t.to(DEVICE) for t in test_tensors]\n",
        "  \n",
        "  preds = [translate(transformer_best, t) for t in tqdm(test_tensors)]\n",
        "  preds = [p.replace(\"--\",\"-\").replace(\"@\", \" ??? \").replace(\"#\", \" \").replace(\"..\",\".\") for p in preds]\n",
        "  return preds\n",
        "\n",
        "def write_preds(predictions,code,transl=True):\n",
        "\n",
        "  ## WRITE GENERATED PREDICTIONS TO FILE IN EXPECTED FORMAT\n",
        "\n",
        "  preds=[]\n",
        "  transcriptions = lang.test_ld[\"transcriptions\"]\n",
        "  translations = lang.test_ld[\"translation\"]\n",
        "  for t, g, l in zip(transcriptions, predictions, translations):\n",
        "    preds.append(f\"\\\\t {t}\")\n",
        "    preds.append(f\"\\g {g}\")\n",
        "    preds.append(f\"\\l {l}\")\n",
        "    preds.append(\" \")\n",
        "\n",
        "  with open(f\"{PRED_PATH}{code}-test-track1.txt\", \"w\", encoding=\"utf-8\") as fout:\n",
        "    fout.write(\"\\n\".join(preds))\n"
      ],
      "metadata": {
        "id": "vquq9gmTUPid"
      },
      "id": "vquq9gmTUPid",
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds = get_predictions(transformer_best, test_tensors)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nPfVd28cAE0a",
        "outputId": "2f16c231-ab2b-48d3-f7ba-26b7f35cf16d"
      },
      "id": "nPfVd28cAE0a",
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 37/37 [00:20<00:00,  1.82it/s]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "T4"
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}